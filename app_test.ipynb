{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from htmldate import find_date\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "from tqdm import tqdm\n",
    "from textblob import TextBlob\n",
    "import openai\n",
    "\n",
    "from html_extractor import *\n",
    "from get_suburls import *\n",
    "from openai_func import *\n",
    "from get_date import *\n",
    "\n",
    "from keyword_extraction import keyword_extractor_paragraph as kep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting sub urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.75it/s]\n",
      "100%|██████████| 6/6 [00:06<00:00,  1.10s/it]\n",
      "100%|██████████| 65/65 [00:55<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed Fetch: 0\n",
      "Splits: 4\n",
      "Tree size: 183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "urls_list = [\"https://www.khaleejtimes.com\" , \"https://www.indiatoday.in\"]\n",
    "urls_list_str = \",\".join(urls_list)\n",
    "\n",
    "keywords = \"gaza,israel,hamas,idf\"\n",
    "\n",
    "scraper = WebScraper2(sub_url_size = 3 , keywords = keywords)\n",
    "                        # Integration with DB will make it faster in future, as fetching is much faster than scrapping.\n",
    "inside_urls, failed_fetch, sub_url_size, total_size = scraper.get_suburls2(urls_list_str)\n",
    "\n",
    "# print(\"Inside URLs:\", inside_urls)\n",
    "print(\"Failed Fetch:\", failed_fetch)\n",
    "print(\"Splits:\", len(inside_urls))\n",
    "print(\"Tree size:\", total_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining sub urls into one single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_urls = [item for sublist in list(inside_urls.values()) for item in sublist]\n",
    "print(len(website_urls))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Date Fetching\n",
    "\n",
    "Need to integrate Mongo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://www.khaleejtimes.com/long-reads/from-instagrammer-to-war-correspondent-how-gaza-crisis-changed-this-palestinians-life',\n",
       " '15-12-2023')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_date_from_url(website_urls[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating {url : html content} dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_html_extracted = get_html(website_urls)\n",
    "url_html_extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword extraction performed on above dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_extracted_html = kep(website_content = url_html_extracted[0], keywords = keywords, filter_by_amount = 60)\n",
    "\n",
    "url_extracted_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting dictionary to list of tuple pairs, for implementation of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_list = [(key,value[:2000]) for key, value in url_extracted_html.items()] # 1000 is temporary until tokenier function is not set up\n",
    "content_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutting the above list fo batches of batch size MAX_CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONTENT = 6\n",
    "\n",
    "content_list_complete = []\n",
    "\n",
    "iterations = len(content_list) // MAX_CONTENT\n",
    "\n",
    "# iterations = 10       ## For testing\n",
    "\n",
    "for i in range(iterations):\n",
    "    sub_content_list = content_list[MAX_CONTENT * i: MAX_CONTENT * (i + 1)]\n",
    "    content_list_complete.append(sub_content_list)\n",
    "\n",
    "# Handle remaining elements after the loop\n",
    "remaining_elements = content_list[MAX_CONTENT * iterations:]\n",
    "if remaining_elements:\n",
    "    iterations += 1\n",
    "    content_list_complete.append(remaining_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_list_complete = content_list_complete[:8]\n",
    "len(content_list_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Openai's api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 out of 20 completed \n",
      "Batch 4 out of 20 completed \n",
      "Batch 7 out of 20 completed \n"
     ]
    }
   ],
   "source": [
    "question = \"Summary of situation in gaza\"\n",
    "\n",
    "response_complete = ''\n",
    "for data_idx in range(0,len(content_list_complete),3):\n",
    "\n",
    "    prompt = f\"\"\" \n",
    "        Data is in the form of tuples inside list: {content_list_complete[data_idx]} \\n\\n\\n \n",
    "        Question: {question} \\n\\n\\n\n",
    "        Method of reply: 100 - 200 word sentences, clear reply,\n",
    "        provide url if neccessary.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    response = get_completion(prompt)\n",
    "    response_complete += response + \"\\n\\n\"\n",
    "    print(f\"Batch {data_idx + 1} out of {iterations} completed \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The situation in Gaza remains dire as the Israel-Hamas war continues. Life-saving aid for Palestinians is piling up at the border, causing delays in the delivery of essential supplies. Trucks from organizations such as UNICEF, Red Cross, and World Health Organization are stranded at the border crossing at Rafah, the only point of entry for relief. The Palestinian people are experiencing unprecedented levels of hunger, with some resorting to taking food directly from trucks out of desperation.\\n\\nThe war has also had a profound impact on the lives of journalists in Gaza. Plestia Alaqad, a 22-year-old Palestinian journalist, had once aimed to showcase the beauty of Gaza through her Instagram posts but found herself reporting on the war instead. The trauma of witnessing the conflict firsthand has left a lasting mark on her.\\n\\nAmidst the crisis, the UAE has been delivering 1.2 million gallons of water daily to Gaza through its desalination mission, providing some relief to the suffering population.\\n\\nThe ongoing war between Israel and Hamas has resulted in significant destruction and loss of life in Gaza. The conflict has also seen the involvement of other nations, with the US issuing a final warning to Houthi rebels in Yemen after a drone attack on ships in the Red Sea. In response, the US sank three ships and killed 10 Houthi rebels. Israel has also been engaged in the conflict, striking down two drones launched by an Iran-backed Iraqi militia and conducting fresh airstrikes in central Gaza, resulting in the deaths of 35 people.\\n\\nThe need for a peaceful resolution to the situation in Gaza is becoming increasingly urgent. The international community continues to monitor developments closely.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2 = get_completion(response_complete)\n",
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Output_gaza.txt\" , \"w\") as f:\n",
    "    f.write(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Output_gaza_summary.txt\" , \"w\") as f:\n",
    "    f.write(response2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

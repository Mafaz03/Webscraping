{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from htmldate import find_date\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "from tqdm import tqdm\n",
    "from textblob import TextBlob\n",
    "\n",
    "from html_extractor import *\n",
    "from get_suburls import *\n",
    "\n",
    "from keyword_extraction import keyword_extractor_paragraph as kep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting sub urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_list = [\"https://www.khaleejtimes.com\"]\n",
    "scraper = WebScraper(1) # KEEP IT 1, 2 or more will result in 1000's of urls.\n",
    "                        # Integration with DB will make it faster in future, as fetching is much faster than scrapping.\n",
    "inside_urls, failed_fetch, sub_url_size, total_size = scraper.get_suburls(urls_list)\n",
    "\n",
    "print(\"Inside URLs:\", inside_urls)\n",
    "print(\"Failed Fetch:\", failed_fetch)\n",
    "print(\"Splits:\", len(inside_urls))\n",
    "print(\"Tree size:\", total_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining sub urls into one single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_urls = [item for sublist in list(inside_urls.values()) for item in sublist]\n",
    "print(len(website_urls))\n",
    "\n",
    "website_urls = website_urls[299:]\n",
    "len(website_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating {url : html content} dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_html_extracted = get_html(website_urls)\n",
    "url_html_extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword extraction performed on above dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"billion\"]\n",
    "url_extracted_html = kep(website_content = url_html_extracted[0], keywords = keywords, filter_by_amount = 60)\n",
    "\n",
    "\n",
    "url_extracted_html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

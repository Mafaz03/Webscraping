{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from htmldate import find_date\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "from tqdm import tqdm\n",
    "from textblob import TextBlob\n",
    "import openai\n",
    "import multiprocessing\n",
    "from time import time\n",
    "\n",
    "from html_extractor import *\n",
    "from get_suburls import *\n",
    "from openai_func import *\n",
    "from get_date import *\n",
    "from parallel import *\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "from keyword_extraction import keyword_extractor_paragraph as kep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting sub urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.68it/s]\n",
      "100%|██████████| 5/5 [00:06<00:00,  1.29s/it]\n",
      "100%|██████████| 66/66 [00:23<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed Fetch: 0\n",
      "Splits: 4\n",
      "Tree size: 181\n"
     ]
    }
   ],
   "source": [
    "urls_list = [\"https://www.khaleejtimes.com\" , \"https://www.indiatoday.in\"]\n",
    "urls_list_str = \",\".join(urls_list)\n",
    "\n",
    "keywords = \"gaza,israel,hamas,idf\"\n",
    "\n",
    "scraper = WebScraper2(sub_url_size = 3 , keywords = keywords)\n",
    "                        # Integration with DB will make it faster in future, as fetching is much faster than scrapping.\n",
    "inside_urls, failed_fetch, sub_url_size, total_size = scraper.get_suburls2(urls_list_str)\n",
    "\n",
    "# print(\"Inside URLs:\", inside_urls)\n",
    "print(\"Failed Fetch:\", failed_fetch)\n",
    "print(\"Splits:\", len(inside_urls))\n",
    "print(\"Tree size:\", total_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining sub urls into one single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181\n"
     ]
    }
   ],
   "source": [
    "website_urls = [item for sublist in list(inside_urls.values()) for item in sublist]\n",
    "print(len(website_urls))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Date Fetching\n",
    "\n",
    "Need to integrate Mongo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://www.indiatoday.in/topic/israel', '12-09-2019')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_date_from_url(website_urls[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating {url : html content} dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 181/181 [00:59<00:00,  3.03it/s]\n"
     ]
    }
   ],
   "source": [
    "url_html_extracted = get_html(website_urls)\n",
    "# url_html_extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword extraction performed on above dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:00<00:00, 8663.04it/s]\n"
     ]
    }
   ],
   "source": [
    "url_extracted_html = kep(website_content = url_html_extracted[0], keywords = keywords, filter_by_amount = 60)\n",
    "\n",
    "# url_extracted_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting dictionary to list of tuple pairs, for implementation of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_list = [(key,value[:2000]) for key, value in url_extracted_html.items()] # 1000 is temporary until tokenier function is not set up\n",
    "# content_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutting the above list fo batches of batch size MAX_CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_CONTENT = 5\n",
    "\n",
    "content_list_complete = []\n",
    "\n",
    "iterations = len(content_list) // MAX_CONTENT\n",
    "\n",
    "\n",
    "for i in range(iterations):\n",
    "    sub_content_list = content_list[MAX_CONTENT * i: MAX_CONTENT * (i + 1)]\n",
    "    content_list_complete.append(sub_content_list)\n",
    "\n",
    "# Handle remaining elements after the loop\n",
    "remaining_elements = content_list[MAX_CONTENT * iterations:]\n",
    "if remaining_elements:\n",
    "    iterations += 1\n",
    "    content_list_complete.append(remaining_elements)\n",
    "\n",
    "len(content_list_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Openai's api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non parallel execution of 1 api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 out of 24 completed \n",
      "Batch 2 out of 24 completed \n",
      "Batch 3 out of 24 completed \n",
      "Batch 4 out of 24 completed \n",
      "Batch 5 out of 24 completed \n",
      "Batch 6 out of 24 completed \n",
      "Batch 7 out of 24 completed \n",
      "Batch 8 out of 24 completed \n",
      "Batch 9 out of 24 completed \n",
      "Batch 10 out of 24 completed \n",
      "Executed in 216.86s\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "start = time()\n",
    "\n",
    "question = \"Summary of situation in gaza\"\n",
    "\n",
    "response_complete = ''\n",
    "for data_idx in range(10):\n",
    "\n",
    "    prompt = f\"\"\" \n",
    "        Data is in the form of tuples inside list: {content_list_complete[data_idx]} \\n\\n\\n \n",
    "        Question: {question} \\n\\n\\n\n",
    "        Method of reply: 100 - 200 word sentences, clear reply,\n",
    "        provide url if neccessary.\n",
    "        \"\"\"\n",
    "    \n",
    "    if data_idx % 6 == 0:\n",
    "        sleep(20)\n",
    "\n",
    "    response = get_completion(prompt)\n",
    "    response_complete += response + \"\\n\\n\"\n",
    "    print(f\"Batch {data_idx + 1} out of {iterations} completed \")\n",
    "\n",
    "end = time()\n",
    "\n",
    "print(f\"Executed in {end-start:.2f}s\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Execution for 2 api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 - 2 executed out of 24\n",
      "Batch 3 - 4 executed out of 24\n",
      "Batch 5 - 6 executed out of 24\n",
      "Batch 7 - 8 executed out of 24\n",
      "Batch 9 - 10 executed out of 24\n",
      "Executed in 98.97s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "complete_result_of_openai = \"\"\n",
    "question_to_pass = \"status of war in gaza\"\n",
    "\n",
    "count = 0\n",
    "\n",
    "NUM_OF_API = 2\n",
    "\n",
    "for i in range(0, 10, NUM_OF_API):\n",
    "\n",
    "    if count % 6 == 0:\n",
    "        sleep(20)\n",
    "    \n",
    "\n",
    "    result = ''\n",
    "\n",
    "    result_queue1 = multiprocessing.Queue()\n",
    "    result_queue2 = multiprocessing.Queue()\n",
    "\n",
    "\n",
    "    process1 = multiprocessing.Process(target=gpt1, args=(question_to_pass, content_list_complete, i, result_queue1))\n",
    "    process2 = multiprocessing.Process(target=gpt2, args=(question_to_pass, content_list_complete, i+1, result_queue2))\n",
    "\n",
    "\n",
    "    # Start processes\n",
    "    process1.start()\n",
    "    process2.start()\n",
    "\n",
    "    # Wait for processes to finish\n",
    "    process1.join()\n",
    "    process2.join()\n",
    "\n",
    "\n",
    "    result1 = result_queue1.get()\n",
    "    result2 = result_queue2.get()\n",
    "\n",
    "    # Rest of your code remains unchanged\n",
    "    result = result1 + \"\\n\\n\" + result2 + \"\\n\\n\"\n",
    "    complete_result_of_openai += result\n",
    "\n",
    "    print(f\"Batch {i+1} - {i+NUM_OF_API} executed out of {len(content_list_complete)}\")\n",
    "    count += 1\n",
    "complete_result_of_openai\n",
    "\n",
    "end = time()\n",
    "\n",
    "print(f\"Executed in {end-start:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.204081632653061"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "216/98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = get_completion(f\"Provide Detailed Summary of {complete_result_of_openai}\")\n",
    "# response3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Output_gaza_parallel.txt\" , \"w\") as f:\n",
    "    f.write(complete_result_of_openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Output_gaza_parallel_summary.txt\" , \"w\") as f:\n",
    "    f.write(complete_result_of_openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Output_gaza.txt\" , \"w\") as f:\n",
    "    f.write(response_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Output_gaza_summary.txt\" , \"w\") as f:\n",
    "    f.write(response2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
